{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliNasirDawood/Allan/blob/master/Sentiment_Analysis_By_LSTM_%26_ROBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEg0552DFJfi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JOxcWdOXBaP"
      },
      "outputs": [],
      "source": [
        "#general purpose packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#data processing\n",
        "import re, string\n",
        "import nltk\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#Naive Bayes\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#transformers\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TFBertModel\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import TFRobertaModel\n",
        "\n",
        "#keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "#metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#set seed for reproducibility\n",
        "seed=42\n",
        "\n",
        "#set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.despine()\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOa1082WuxK8"
      },
      "outputs": [],
      "source": [
        "def conf_matrix(y, y_pred, title):\n",
        "    fig, ax =plt.subplots(figsize=(5,5))\n",
        "    labels=['Negative', 'Neutral', 'Positive']\n",
        "    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n",
        "    plt.title(title, fontsize=20)\n",
        "    ax.xaxis.set_ticklabels(labels, fontsize=17) \n",
        "    ax.yaxis.set_ticklabels(labels, fontsize=17)\n",
        "    ax.set_ylabel('Test', fontsize=20)\n",
        "    ax.set_xlabel('Predicted', fontsize=20)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JycEGhBJuxNh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/twitter data/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/twitter data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD6PcK0wuxP_"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA8aWEZluxSu"
      },
      "outputs": [],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_mbqZbeuxVT"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DBDs4LYuxX6"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(subset='text',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuL3I2FFuxae"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCuoG88wuxdV"
      },
      "outputs": [],
      "source": [
        "df = df[['text','sentiment']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp5wHw9PuxgV"
      },
      "outputs": [],
      "source": [
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df['text'] = df['text'].apply(lambda x: x.lower())\n",
        "df['text'] = df['text'].apply((lambda x: re.sub('[^\\w\\s]','',x)))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pzjdyy5uxjB"
      },
      "outputs": [],
      "source": [
        "df_test[\"text\"] = df_test[\"text\"].astype(str)\n",
        "df_test['text'] = df_test['text'].apply(lambda x: x.lower())\n",
        "df_test['text'] = df_test['text'].apply((lambda x: re.sub('[^\\w\\s]','',x)))\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1azJDwvyuxmZ"
      },
      "outputs": [],
      "source": [
        "text_len = []\n",
        "for text in df.text:\n",
        "    tweet_len = len(text.split())\n",
        "    text_len.append(tweet_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdRXEDFNKgAC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjgWcHmQvQ-l"
      },
      "outputs": [],
      "source": [
        "df['text_len'] = text_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F79bvMuvvRBJ"
      },
      "outputs": [],
      "source": [
        "text_len_test = []\n",
        "for text in df_test.text:\n",
        "    tweet_len = len(text.split())\n",
        "    text_len_test.append(tweet_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKy2x4S6vRDy"
      },
      "outputs": [],
      "source": [
        "df_test['text_len'] = text_len_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRQLDc63vRGe"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n",
        "plt.title('Training tweets with less than 10 words')\n",
        "plt.yticks([])\n",
        "#ax.bar_label(ax.containers[0])\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQFhGIX2vRJG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "ax = sns.countplot(x='text_len', data=df_test[df_test['text_len']<10], palette='mako')\n",
        "plt.title('Test tweets with less than 10 words')\n",
        "plt.yticks([])\n",
        "#ax.bar_label(ax.containers[0])\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7eRnZLcvRLz"
      },
      "outputs": [],
      "source": [
        "print(f\" DF SHAPE: {df.shape}\")\n",
        "print(f\" DF TEST SHAPE: {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PHJz0eNvROc"
      },
      "outputs": [],
      "source": [
        "df_test = df_test[df_test['text_len'] > 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6zyMF6DvRRN"
      },
      "outputs": [],
      "source": [
        "print(f\" DF SHAPE: {df.shape}\")\n",
        "print(f\" DF TEST SHAPE: {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fl74vm_vRUB"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD3LBpYCvRWw"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df['text'].values:\n",
        "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens.append(len(tokens))\n",
        "    \n",
        "max_len=np.max(token_lens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCW-pV4xvRZp"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjFddJBivRcc"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for i,txt in enumerate(df['text'].values):\n",
        "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens.append(len(tokens))\n",
        "    if len(tokens)>80:\n",
        "        print(f\"INDEX: {i}, TEXT: {txt}\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQdhAC2qvRfE"
      },
      "outputs": [],
      "source": [
        "df['token_lens'] = token_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7_u6A2uvRhr"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by='token_lens', ascending=False)\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5Evg1a6vRkD"
      },
      "outputs": [],
      "source": [
        "df = df.iloc[12:]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5jgFvaivRnC"
      },
      "outputs": [],
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO5b6wr3vRpe"
      },
      "outputs": [],
      "source": [
        "token_lens_test = []\n",
        "\n",
        "for txt in df_test['text'].values:\n",
        "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens_test.append(len(tokens))\n",
        "    \n",
        "max_len=np.max(token_lens_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hscPUupSvRsM"
      },
      "outputs": [],
      "source": [
        "print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kalJ7zmOvRu0"
      },
      "outputs": [],
      "source": [
        "token_lens_test = []\n",
        "\n",
        "for i,txt in enumerate(df_test['text'].values):\n",
        "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens_test.append(len(tokens))\n",
        "    if len(tokens)>80:\n",
        "        print(f\"INDEX: {i}, TEXT: {txt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohXNq241vRxd"
      },
      "outputs": [],
      "source": [
        "df_test['token_lens'] = token_lens_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iepGxF5XvR0G"
      },
      "outputs": [],
      "source": [
        "df_test = df_test.sort_values(by='token_lens', ascending=False)\n",
        "df_test.head(10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLMnOZMzvR2k"
      },
      "outputs": [],
      "source": [
        "df_test = df_test.iloc[5:]\n",
        "df_test.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ_xxwXgvR5K"
      },
      "outputs": [],
      "source": [
        "df_test = df_test.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HCFidTJvR7x"
      },
      "outputs": [],
      "source": [
        "df['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovbKOKLXvR-0"
      },
      "outputs": [],
      "source": [
        "df['Sentiment'] = df['sentiment'].map({'Negative':0,'Neutral':1,'Positive':2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km7hyZwlvSCJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_test['Sentiment'] = df_test['sentiment'].map({'Negative':0,'Neutral':1,'Positive':2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKTnnlAZI5lA"
      },
      "outputs": [],
      "source": [
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRs4iPJIwHLG"
      },
      "outputs": [],
      "source": [
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRWYw853wHNr"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler()\n",
        "train_x, train_y = ros.fit_resample(np.array(df['text']).reshape(-1, 1), np.array(df['sentiment']).reshape(-1, 1));\n",
        "train_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text', 'sentiment']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxqSs3cEwHP2"
      },
      "outputs": [],
      "source": [
        "train_os['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYtTxJ4vwHSk"
      },
      "outputs": [],
      "source": [
        "X = train_os['text'].values\n",
        "y = train_os['sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8ycwtSiwHVE"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID1EMLunwHX-"
      },
      "outputs": [],
      "source": [
        "X_test = df_test['text'].values\n",
        "y_test = df_test['sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF41oP2EwHaQ"
      },
      "outputs": [],
      "source": [
        "y_train_le = y_train.copy()\n",
        "y_valid_le = y_valid.copy()\n",
        "y_test_le = y_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9FL-pdXwHdI"
      },
      "outputs": [],
      "source": [
        "ohe = preprocessing.OneHotEncoder()\n",
        "y_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
        "y_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\n",
        "y_test = ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glLL6DJtwHfr"
      },
      "outputs": [],
      "source": [
        "print(f\"TRAINING DATA: {X_train.shape[0]}\\nVALIDATION DATA: {X_valid.shape[0]}\\nTESTING DATA: {X_test.shape[0]}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn8KmrAiwHiw"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFLCwMA0wHk_"
      },
      "outputs": [],
      "source": [
        "def tokenize(data,max_len=MAX_LEN) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            data[i],\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INOLR_-UwHno"
      },
      "outputs": [],
      "source": [
        "train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN)\n",
        "val_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\n",
        "test_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNjkQKYCwHqb"
      },
      "outputs": [],
      "source": [
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vavKjXEZwHs_"
      },
      "outputs": [],
      "source": [
        "def create_model(bert_model, max_len=MAX_LEN):\n",
        "    \n",
        "    ##params###\n",
        "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5, decay=1e-7)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "    accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "\n",
        "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    \n",
        "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    \n",
        "    embeddings = bert_model([input_ids,attention_masks])[1]\n",
        "    \n",
        "    output = tf.keras.layers.Dense(3, activation=\"softmax\")(embeddings)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n",
        "    \n",
        "    model.compile(opt, loss=loss, metrics=accuracy)\n",
        "    \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEHyc10GwHvn"
      },
      "outputs": [],
      "source": [
        "model = create_model(bert_model, MAX_LEN)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mLYhgrKwHyL"
      },
      "outputs": [],
      "source": [
        "history_bert = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=5, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYEH-3a3wH0z"
      },
      "outputs": [],
      "source": [
        "result_bert = model.predict([test_input_ids,test_attention_masks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9nP81tqwH3X"
      },
      "outputs": [],
      "source": [
        "y_pred_bert =  np.zeros_like(result_bert)\n",
        "y_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuJKFKhwwH6I"
      },
      "outputs": [],
      "source": [
        "conf_matrix(y_test.argmax(1), y_pred_bert.argmax(1),'BERT Sentiment Analysis\\nConfusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iikV1JyrwH9B"
      },
      "outputs": [],
      "source": [
        "print('\\tClassification Report for BERT:\\n\\n',classification_report(y_test,y_pred_bert, target_names=['Negative', 'Neutral', 'Positive']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98FJKSxUwIAB"
      },
      "outputs": [],
      "source": [
        "tokenizer_roberta = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnARbMABwICn"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in X_train:\n",
        "    tokens = tokenizer_roberta.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens.append(len(tokens))\n",
        "max_length=np.max(token_lens)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnAZqRfywIFK"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku8TbwaFZTy_"
      },
      "outputs": [],
      "source": [
        "def tokenize_roberta(data,max_len=MAX_LEN) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer_roberta.encode_plus(\n",
        "            data[i],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rorWU_okZT1u"
      },
      "outputs": [],
      "source": [
        "train_input_ids, train_attention_masks = tokenize_roberta(X_train, MAX_LEN)\n",
        "val_input_ids, val_attention_masks = tokenize_roberta(X_valid, MAX_LEN)\n",
        "test_input_ids, test_attention_masks = tokenize_roberta(X_test, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GvIwcDNZT4q"
      },
      "outputs": [],
      "source": [
        "def create_model(bert_model, max_len=MAX_LEN):\n",
        "    \n",
        "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5, decay=1e-7)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "    accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    output = bert_model([input_ids,attention_masks])\n",
        "    output = output[1]\n",
        "    output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
        "    model.compile(opt, loss=loss, metrics=accuracy)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QY4afrXZT7h"
      },
      "outputs": [],
      "source": [
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tr2jbQJZT95"
      },
      "outputs": [],
      "source": [
        "model = create_model(roberta_model, MAX_LEN)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYW2zHlLZUA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ef586b-660c-4e4c-d9db-3dd41d62c13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "  21/1001 [..............................] - ETA: 11:25:30 - loss: 1.0992 - categorical_accuracy: 0.3968"
          ]
        }
      ],
      "source": [
        "history_2 = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=5, batch_size=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leYPbg2iZUDl"
      },
      "outputs": [],
      "source": [
        "result_roberta = model.predict([test_input_ids,test_attention_masks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd1hsXDWZUGc"
      },
      "outputs": [],
      "source": [
        "y_pred_roberta =  np.zeros_like(result_roberta)\n",
        "y_pred_roberta[np.arange(len(y_pred_roberta)), result_roberta.argmax(1)] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEl8-yFVZUJZ"
      },
      "outputs": [],
      "source": [
        "conf_matrix(y_test.argmax(1),y_pred_roberta.argmax(1),'RoBERTa Sentiment Analysis\\nConfusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foR33LwcZUMI"
      },
      "outputs": [],
      "source": [
        "print('\\tClassification Report for RoBERTa:\\n\\n',classification_report(y_test,y_pred_roberta, target_names=['Negative', 'Neutral', 'Positive']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G68iTyFzZUPS"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for BERT:\\n',classification_report(y_test,y_pred_bert, target_names=['Negative', 'Neutral', 'Positive']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GctKOR-tZURu"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for RoBERTa:\\n',classification_report(y_test,y_pred_roberta, target_names=['Negative', 'Neutral', 'Positive']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2t17rcBZUU8"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(9,5.5))\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "plt.suptitle('Sentiment Analysis Comparison\\n Confusion Matrix', fontsize=20)\n",
        "\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test.argmax(1),y_pred_bert.argmax(1)), annot=True, cmap=\"Blues\", fmt='g', cbar=False, ax=ax[0], annot_kws={\"size\":25})\n",
        "\n",
        "ax[0].set_title('BERT Classifier', fontsize=20)\n",
        "ax[0].set_yticklabels(labels, fontsize=17);\n",
        "ax[0].set_xticklabels(labels, fontsize=17);\n",
        "ax[0].set_ylabel('Test', fontsize=20)\n",
        "ax[0].set_xlabel('Predicted', fontsize=20)\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test.argmax(1),y_pred_roberta.argmax(1)), annot=True, cmap=\"Blues\", fmt='g', cbar=False, ax=ax[1], annot_kws={\"size\":25})\n",
        "ax[1].set_title('RoBERTa Classifier', fontsize=20)\n",
        "ax[1].set_yticklabels(labels, fontsize=17);\n",
        "ax[1].set_xticklabels(labels, fontsize=17);\n",
        "ax[1].set_ylabel('Test', fontsize=20)\n",
        "ax[1].set_xlabel('Predicted', fontsize=20)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1WWzVnKZUXu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnKW0yo6ZUaP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w2Lnl0kZUc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSV-CUaSZUgE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w17X-n2wZUjG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX-CJyLAZUmn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LkTS-ahwIIE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llIUD0TiwIK8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVGHqz8EKxTB"
      },
      "outputs": [],
      "source": [
        "def text_cleaning(text):\n",
        "  text = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
        "                          \" \",          # Replace all non-letters with spaces\n",
        "                          str(text))\n",
        "  text = re.sub(r'@[A-Za-z0-9]+', '', text)     # removing @mentions\n",
        "  text = re.sub(r'@[A-Za-zA-Z0-9]+', '', text)  # removing @mentions \n",
        "  text = re.sub(r'@[A-Za-z]+', '', text)        # removing @mentions\n",
        "  text = re.sub(r'@[-)]+', '', text)            # removing @mentions\n",
        "  text = re.sub(r'#', '', text )                # removing '#' sign\n",
        "  text = re.sub(r'RT[\\s]+', '', text)           # removing RT\n",
        "  text = re.sub(r'https?\\/\\/\\S+', '', text)     # removing the hyper link\n",
        "  text = re.sub(r'&[a-z;]+', '', text)          # removing '&gt;'\n",
        "\n",
        "  return text\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(text_cleaning)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cfeqioe5Lrrt"
      },
      "outputs": [],
      "source": [
        "df['category'].mask(df['category'] == -1,'negative',  inplace=True)\n",
        "df['category'].mask(df['category'] == 0,'normal',  inplace=True)\n",
        "df['category'].mask(df['category'] == 1,'positive',  inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSvqqLTnXmlJ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVutv3UPXrZ5"
      },
      "outputs": [],
      "source": [
        "df.category.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43VNM-XPXvSn"
      },
      "outputs": [],
      "source": [
        "df= df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzXsN6bAXxvq"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onhGGZgsXxyY"
      },
      "outputs": [],
      "source": [
        "df['category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22HVBaOeXx07"
      },
      "outputs": [],
      "source": [
        "dist = df['category'].value_counts()\n",
        "import plotly.graph_objects as go\n",
        "def ditribution_plot(x,y,name):\n",
        "    fig = go.Figure([\n",
        "        go.Bar(x=x, y=y)\n",
        "    ])\n",
        "\n",
        "    fig.update_layout(title_text=name)\n",
        "    fig.show()\n",
        "ditribution_plot(x= dist.index, y= dist.values, name= 'Class Distribution train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrP99D9cXx4B"
      },
      "outputs": [],
      "source": [
        "def clean_text(df, field):\n",
        "    df[field] = df[field].str.replace(r\"@\",\" at \")\n",
        "    df[field] = df[field].str.replace(\"#[^a-zA-Z0-9_]+\",\" \")\n",
        "    df[field] = df[field].str.replace(r\"[^a-zA-Z(),\\\"'\\n_]\",\" \")\n",
        "    df[field] = df[field].str.replace(r\"http\\S+\",\"\")\n",
        "    df[field] = df[field].str.lower()\n",
        "    return df\n",
        "\n",
        "clean_text(df,'clean_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipx8ks8vXx6z"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# stem = PorterStemmer()\n",
        "# df['clean_text'] = [stem.stem(word) for word in df['clean_text'] if not word in stopwords.words('english')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueR4wKOHMjXB"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['clean_text'])\n",
        "df['clean_text'] = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "df['clean_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xariSOKFXqhD"
      },
      "outputs": [],
      "source": [
        "df['category'].mask(df['category'] == 'negative',-1,  inplace=True)\n",
        "df['category'].mask(df['category'] == 'normal',0,  inplace=True)\n",
        "df['category'].mask(df['category'] == 'positive',1,  inplace=True)\n",
        "df['category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-vw8oTwPIml"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'],df['category'], test_size=0.2, random_state=40)\n",
        "print('X_train:',len(X_train))\n",
        "print('y_train:',len(y_train))\n",
        "print('X_test:',len(X_test))\n",
        "print('y_test:',len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHtvJEYlPMC-"
      },
      "outputs": [],
      "source": [
        "X_train = pad_sequences( X_train, maxlen=100 ,dtype='float32')\n",
        "X_test = pad_sequences( X_test, maxlen=100 ,dtype='float32')\n",
        "len(tokenizer.index_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07dVPgssPOED"
      },
      "outputs": [],
      "source": [
        "model_1 = Sequential()\n",
        "model_1.add(Embedding(len(tokenizer.index_word)+1, input_length= 100 ,output_dim =50))\n",
        "model_1.add(Bidirectional(LSTM(100)))\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(250, activation='relu'))\n",
        "model_1.add(Dropout(0.2))\n",
        "model_1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_1.compile(\"adam\", loss= 'binary_crossentropy' ,metrics=[\"accuracy\"])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBZTIHR7PSHN"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\",patience=5,verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgUKK0sVPyQy"
      },
      "outputs": [],
      "source": [
        "X_train = np.asarray(X_train).astype(np.float32)\n",
        "X_test = np.asarray(X_test).astype(np.float32)\n",
        "\n",
        "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
        "y_test = np.asarray(y_test).astype('float32').reshape((-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wYseactP0qz"
      },
      "outputs": [],
      "source": [
        "history_1=model_1.fit(X_train , y_train ,batch_size=64, epochs=7,\n",
        "                    validation_data=(X_test , y_test),callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXXIgZsEY742"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.index_word)+1, input_length= 100 ,output_dim =100))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(\"adam\", loss= 'categorical_crossentropy' ,metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ynupjqJZcRl"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train , y_train ,batch_size=256, epochs=4,\n",
        "                    validation_data=(X_test , y_test),callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oot99FxZe09"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(X_test, y_test, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6L-d6k9ZgyY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VTKSjLEhZxlptazGnbl0dV8Mm7lpmFpn",
      "authorship_tag": "ABX9TyOJ1wjf4gdtKeKCr56TUfGi",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}